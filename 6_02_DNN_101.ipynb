{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnzilShahjahan/AnzilShahjahan--Data-Science---Gen-AI-Submission/blob/main/6_02_DNN_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "d18b136e-68cd-45d9-81bd-4932f857e9e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "7da7e8c6-d0a9-4a66-890a-ec403537d223"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "e359795a-8f95-4bd9-9278-b73ea2c352f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # Added an extra dense layer\n",
        "            nn.Linear(5, 5),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "ce6828a6-fef6-48ec-e997-ccbcd98eab66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 29340.6875\n",
            "Epoch [10/100], Loss: 29209.0098\n",
            "Epoch [10/100], Loss: 24242.2637\n",
            "Epoch [10/100], Loss: 30597.8516\n",
            "Epoch [10/100], Loss: 29974.3027\n",
            "Epoch [10/100], Loss: 29350.4102\n",
            "Epoch [10/100], Loss: 32480.9551\n",
            "Epoch [10/100], Loss: 56729.7031\n",
            "Epoch [20/100], Loss: 32060.4844\n",
            "Epoch [20/100], Loss: 34058.5781\n",
            "Epoch [20/100], Loss: 26668.6367\n",
            "Epoch [20/100], Loss: 25340.1387\n",
            "Epoch [20/100], Loss: 29869.5742\n",
            "Epoch [20/100], Loss: 30728.8984\n",
            "Epoch [20/100], Loss: 27821.127\n",
            "Epoch [20/100], Loss: 25283.8789\n",
            "Epoch [30/100], Loss: 32408.6641\n",
            "Epoch [30/100], Loss: 33560.7852\n",
            "Epoch [30/100], Loss: 25649.2891\n",
            "Epoch [30/100], Loss: 26018.0176\n",
            "Epoch [30/100], Loss: 34282.8633\n",
            "Epoch [30/100], Loss: 22301.1191\n",
            "Epoch [30/100], Loss: 31269.4238\n",
            "Epoch [30/100], Loss: 29677.625\n",
            "Epoch [40/100], Loss: 31622.5801\n",
            "Epoch [40/100], Loss: 26894.8965\n",
            "Epoch [40/100], Loss: 29979.0176\n",
            "Epoch [40/100], Loss: 24610.0352\n",
            "Epoch [40/100], Loss: 26986.6602\n",
            "Epoch [40/100], Loss: 34639.1406\n",
            "Epoch [40/100], Loss: 29483.1348\n",
            "Epoch [40/100], Loss: 31765.5059\n",
            "Epoch [50/100], Loss: 26706.6094\n",
            "Epoch [50/100], Loss: 28020.8652\n",
            "Epoch [50/100], Loss: 30507.2871\n",
            "Epoch [50/100], Loss: 27979.5547\n",
            "Epoch [50/100], Loss: 29102.1387\n",
            "Epoch [50/100], Loss: 30307.0215\n",
            "Epoch [50/100], Loss: 29046.5293\n",
            "Epoch [50/100], Loss: 44913.375\n",
            "Epoch [60/100], Loss: 28600.416\n",
            "Epoch [60/100], Loss: 19852.707\n",
            "Epoch [60/100], Loss: 26381.8164\n",
            "Epoch [60/100], Loss: 28283.0898\n",
            "Epoch [60/100], Loss: 32684.2637\n",
            "Epoch [60/100], Loss: 28690.1152\n",
            "Epoch [60/100], Loss: 33885.5273\n",
            "Epoch [60/100], Loss: 30030.5742\n",
            "Epoch [70/100], Loss: 20286.3438\n",
            "Epoch [70/100], Loss: 33144.5039\n",
            "Epoch [70/100], Loss: 21522.4395\n",
            "Epoch [70/100], Loss: 27711.2988\n",
            "Epoch [70/100], Loss: 30422.3984\n",
            "Epoch [70/100], Loss: 29363.7148\n",
            "Epoch [70/100], Loss: 29753.0898\n",
            "Epoch [70/100], Loss: 26164.3262\n",
            "Epoch [80/100], Loss: 30103.9375\n",
            "Epoch [80/100], Loss: 28458.2441\n",
            "Epoch [80/100], Loss: 25272.1602\n",
            "Epoch [80/100], Loss: 21769.9473\n",
            "Epoch [80/100], Loss: 25889.2754\n",
            "Epoch [80/100], Loss: 22629.6191\n",
            "Epoch [80/100], Loss: 30127.6387\n",
            "Epoch [80/100], Loss: 17507.9961\n",
            "Epoch [90/100], Loss: 31487.9512\n",
            "Epoch [90/100], Loss: 24578.6992\n",
            "Epoch [90/100], Loss: 22014.0195\n",
            "Epoch [90/100], Loss: 33450.3984\n",
            "Epoch [90/100], Loss: 19041.2539\n",
            "Epoch [90/100], Loss: 23543.7363\n",
            "Epoch [90/100], Loss: 20031.4473\n",
            "Epoch [90/100], Loss: 10483.8066\n",
            "Epoch [100/100], Loss: 26836.9141\n",
            "Epoch [100/100], Loss: 22679.7305\n",
            "Epoch [100/100], Loss: 20100.1816\n",
            "Epoch [100/100], Loss: 27193.2598\n",
            "Epoch [100/100], Loss: 20508.002\n",
            "Epoch [100/100], Loss: 24274.5898\n",
            "Epoch [100/100], Loss: 20640.418\n",
            "Epoch [100/100], Loss: 5473.8662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "83d82382-bafe-4d68-c0b2-d18d187bcee0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 19914.01171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "f48b7a9f-b172-451b-dbde-45ae63bdf1df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0   24.761438   219.0\n",
              "1   23.018602    70.0\n",
              "2   24.213774   202.0\n",
              "3   28.994030   230.0\n",
              "4   23.393604   111.0\n",
              "..        ...     ...\n",
              "84  21.512604   153.0\n",
              "85  19.979057    98.0\n",
              "86  18.106094    37.0\n",
              "87  18.731207    63.0\n",
              "88  21.739443   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-23911b54-ea96-449e-a3be-a6ed2557cfb0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24.761438</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23.018602</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24.213774</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28.994030</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23.393604</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>21.512604</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>19.979057</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>18.106094</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>18.731207</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>21.739443</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23911b54-ea96-449e-a3be-a6ed2557cfb0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-23911b54-ea96-449e-a3be-a6ed2557cfb0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-23911b54-ea96-449e-a3be-a6ed2557cfb0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_48173adc-f213-436f-b401-d7fe42abffe8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_48173adc-f213-436f-b401-d7fe42abffe8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          24.00196647644043,\n          21.251476287841797,\n          24.01321029663086\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load data\n",
        "data = load_diabetes()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop with 1000 epochs\n",
        "epochs = 1000 # 1000 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4XcBmpV7hvI",
        "outputId": "dc7be962-3da7-43ef-dc60-a4fb500cb706"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 3767.5476\n",
            "Epoch [10/1000], Loss: 2772.3696\n",
            "Epoch [10/1000], Loss: 3139.7856\n",
            "Epoch [10/1000], Loss: 2692.1602\n",
            "Epoch [10/1000], Loss: 2431.3672\n",
            "Epoch [10/1000], Loss: 2662.7012\n",
            "Epoch [10/1000], Loss: 2867.3381\n",
            "Epoch [10/1000], Loss: 2884.8584\n",
            "Epoch [20/1000], Loss: 2050.7666\n",
            "Epoch [20/1000], Loss: 2112.4966\n",
            "Epoch [20/1000], Loss: 2312.6243\n",
            "Epoch [20/1000], Loss: 3044.2666\n",
            "Epoch [20/1000], Loss: 4285.0522\n",
            "Epoch [20/1000], Loss: 3039.4746\n",
            "Epoch [20/1000], Loss: 3173.3896\n",
            "Epoch [20/1000], Loss: 8349.8906\n",
            "Epoch [30/1000], Loss: 2781.5686\n",
            "Epoch [30/1000], Loss: 3124.9072\n",
            "Epoch [30/1000], Loss: 3146.7009\n",
            "Epoch [30/1000], Loss: 2804.9839\n",
            "Epoch [30/1000], Loss: 2795.4377\n",
            "Epoch [30/1000], Loss: 2380.6038\n",
            "Epoch [30/1000], Loss: 3391.6921\n",
            "Epoch [30/1000], Loss: 1503.2627\n",
            "Epoch [40/1000], Loss: 2914.0161\n",
            "Epoch [40/1000], Loss: 2759.1948\n",
            "Epoch [40/1000], Loss: 3108.5356\n",
            "Epoch [40/1000], Loss: 2982.4119\n",
            "Epoch [40/1000], Loss: 2814.6206\n",
            "Epoch [40/1000], Loss: 3175.1162\n",
            "Epoch [40/1000], Loss: 2563.2512\n",
            "Epoch [40/1000], Loss: 3256.0896\n",
            "Epoch [50/1000], Loss: 2376.0432\n",
            "Epoch [50/1000], Loss: 2347.2319\n",
            "Epoch [50/1000], Loss: 3562.6631\n",
            "Epoch [50/1000], Loss: 3438.7805\n",
            "Epoch [50/1000], Loss: 2487.6772\n",
            "Epoch [50/1000], Loss: 3356.0498\n",
            "Epoch [50/1000], Loss: 2798.5515\n",
            "Epoch [50/1000], Loss: 2212.6489\n",
            "Epoch [60/1000], Loss: 3129.3953\n",
            "Epoch [60/1000], Loss: 3323.1201\n",
            "Epoch [60/1000], Loss: 2289.2671\n",
            "Epoch [60/1000], Loss: 3063.4753\n",
            "Epoch [60/1000], Loss: 2769.158\n",
            "Epoch [60/1000], Loss: 2864.4465\n",
            "Epoch [60/1000], Loss: 2978.0027\n",
            "Epoch [60/1000], Loss: 1581.5635\n",
            "Epoch [70/1000], Loss: 2527.5098\n",
            "Epoch [70/1000], Loss: 2859.6892\n",
            "Epoch [70/1000], Loss: 2397.0562\n",
            "Epoch [70/1000], Loss: 3953.1133\n",
            "Epoch [70/1000], Loss: 2716.4128\n",
            "Epoch [70/1000], Loss: 2665.6121\n",
            "Epoch [70/1000], Loss: 3323.3755\n",
            "Epoch [70/1000], Loss: 984.7924\n",
            "Epoch [80/1000], Loss: 3178.2124\n",
            "Epoch [80/1000], Loss: 2059.0552\n",
            "Epoch [80/1000], Loss: 3173.3652\n",
            "Epoch [80/1000], Loss: 3866.2412\n",
            "Epoch [80/1000], Loss: 2311.0303\n",
            "Epoch [80/1000], Loss: 2775.0181\n",
            "Epoch [80/1000], Loss: 2888.3025\n",
            "Epoch [80/1000], Loss: 4137.2725\n",
            "Epoch [90/1000], Loss: 3059.7881\n",
            "Epoch [90/1000], Loss: 3102.3806\n",
            "Epoch [90/1000], Loss: 3622.7456\n",
            "Epoch [90/1000], Loss: 2095.5037\n",
            "Epoch [90/1000], Loss: 2755.1206\n",
            "Epoch [90/1000], Loss: 3229.8242\n",
            "Epoch [90/1000], Loss: 2579.1538\n",
            "Epoch [90/1000], Loss: 858.8772\n",
            "Epoch [100/1000], Loss: 2420.3738\n",
            "Epoch [100/1000], Loss: 3268.979\n",
            "Epoch [100/1000], Loss: 2704.6255\n",
            "Epoch [100/1000], Loss: 3732.6731\n",
            "Epoch [100/1000], Loss: 2249.0137\n",
            "Epoch [100/1000], Loss: 3546.7092\n",
            "Epoch [100/1000], Loss: 2519.4553\n",
            "Epoch [100/1000], Loss: 925.4408\n",
            "Epoch [110/1000], Loss: 2798.6375\n",
            "Epoch [110/1000], Loss: 2337.0439\n",
            "Epoch [110/1000], Loss: 2950.0674\n",
            "Epoch [110/1000], Loss: 2617.5286\n",
            "Epoch [110/1000], Loss: 3941.0156\n",
            "Epoch [110/1000], Loss: 2337.1506\n",
            "Epoch [110/1000], Loss: 2945.1912\n",
            "Epoch [110/1000], Loss: 9442.0107\n",
            "Epoch [120/1000], Loss: 2513.5117\n",
            "Epoch [120/1000], Loss: 2901.8994\n",
            "Epoch [120/1000], Loss: 3493.9624\n",
            "Epoch [120/1000], Loss: 2656.354\n",
            "Epoch [120/1000], Loss: 3248.0439\n",
            "Epoch [120/1000], Loss: 2489.6011\n",
            "Epoch [120/1000], Loss: 2835.9893\n",
            "Epoch [120/1000], Loss: 5920.8223\n",
            "Epoch [130/1000], Loss: 2528.2756\n",
            "Epoch [130/1000], Loss: 3292.917\n",
            "Epoch [130/1000], Loss: 3086.8352\n",
            "Epoch [130/1000], Loss: 2492.6545\n",
            "Epoch [130/1000], Loss: 3612.0967\n",
            "Epoch [130/1000], Loss: 2999.168\n",
            "Epoch [130/1000], Loss: 2440.1382\n",
            "Epoch [130/1000], Loss: 644.8308\n",
            "Epoch [140/1000], Loss: 2462.918\n",
            "Epoch [140/1000], Loss: 3394.5417\n",
            "Epoch [140/1000], Loss: 3218.4905\n",
            "Epoch [140/1000], Loss: 2881.624\n",
            "Epoch [140/1000], Loss: 3049.4883\n",
            "Epoch [140/1000], Loss: 2694.1575\n",
            "Epoch [140/1000], Loss: 2657.9976\n",
            "Epoch [140/1000], Loss: 2400.8289\n",
            "Epoch [150/1000], Loss: 2663.0195\n",
            "Epoch [150/1000], Loss: 3837.5281\n",
            "Epoch [150/1000], Loss: 3440.5505\n",
            "Epoch [150/1000], Loss: 2664.5017\n",
            "Epoch [150/1000], Loss: 3031.5315\n",
            "Epoch [150/1000], Loss: 1930.5312\n",
            "Epoch [150/1000], Loss: 2297.5869\n",
            "Epoch [150/1000], Loss: 10502.3398\n",
            "Epoch [160/1000], Loss: 2410.8621\n",
            "Epoch [160/1000], Loss: 3242.4407\n",
            "Epoch [160/1000], Loss: 2750.6707\n",
            "Epoch [160/1000], Loss: 3897.4231\n",
            "Epoch [160/1000], Loss: 2941.135\n",
            "Epoch [160/1000], Loss: 2461.5388\n",
            "Epoch [160/1000], Loss: 2663.1689\n",
            "Epoch [160/1000], Loss: 2233.6528\n",
            "Epoch [170/1000], Loss: 3033.2156\n",
            "Epoch [170/1000], Loss: 3099.0217\n",
            "Epoch [170/1000], Loss: 3085.3711\n",
            "Epoch [170/1000], Loss: 3323.2793\n",
            "Epoch [170/1000], Loss: 2707.0925\n",
            "Epoch [170/1000], Loss: 2678.4866\n",
            "Epoch [170/1000], Loss: 2190.3247\n",
            "Epoch [170/1000], Loss: 6217.7217\n",
            "Epoch [180/1000], Loss: 3027.2202\n",
            "Epoch [180/1000], Loss: 3316.9312\n",
            "Epoch [180/1000], Loss: 2343.1182\n",
            "Epoch [180/1000], Loss: 1925.125\n",
            "Epoch [180/1000], Loss: 2944.3528\n",
            "Epoch [180/1000], Loss: 4145.5415\n",
            "Epoch [180/1000], Loss: 2464.1636\n",
            "Epoch [180/1000], Loss: 5353.4517\n",
            "Epoch [190/1000], Loss: 2947.3069\n",
            "Epoch [190/1000], Loss: 2699.8057\n",
            "Epoch [190/1000], Loss: 2605.4619\n",
            "Epoch [190/1000], Loss: 2594.7983\n",
            "Epoch [190/1000], Loss: 3560.8999\n",
            "Epoch [190/1000], Loss: 2794.5203\n",
            "Epoch [190/1000], Loss: 3076.2905\n",
            "Epoch [190/1000], Loss: 3478.5742\n",
            "Epoch [200/1000], Loss: 3980.8438\n",
            "Epoch [200/1000], Loss: 2029.5491\n",
            "Epoch [200/1000], Loss: 2831.5791\n",
            "Epoch [200/1000], Loss: 3080.2864\n",
            "Epoch [200/1000], Loss: 1837.1685\n",
            "Epoch [200/1000], Loss: 3392.9045\n",
            "Epoch [200/1000], Loss: 3149.4336\n",
            "Epoch [200/1000], Loss: 3292.2354\n",
            "Epoch [210/1000], Loss: 2851.0457\n",
            "Epoch [210/1000], Loss: 3096.0581\n",
            "Epoch [210/1000], Loss: 2769.4663\n",
            "Epoch [210/1000], Loss: 2547.6558\n",
            "Epoch [210/1000], Loss: 3334.3291\n",
            "Epoch [210/1000], Loss: 2730.1162\n",
            "Epoch [210/1000], Loss: 2917.0005\n",
            "Epoch [210/1000], Loss: 4049.4189\n",
            "Epoch [220/1000], Loss: 4033.1899\n",
            "Epoch [220/1000], Loss: 2866.7551\n",
            "Epoch [220/1000], Loss: 2591.4336\n",
            "Epoch [220/1000], Loss: 2987.8381\n",
            "Epoch [220/1000], Loss: 2490.4534\n",
            "Epoch [220/1000], Loss: 2782.7637\n",
            "Epoch [220/1000], Loss: 2614.4646\n",
            "Epoch [220/1000], Loss: 2051.8748\n",
            "Epoch [230/1000], Loss: 2744.3481\n",
            "Epoch [230/1000], Loss: 3139.5281\n",
            "Epoch [230/1000], Loss: 2928.3169\n",
            "Epoch [230/1000], Loss: 3003.5469\n",
            "Epoch [230/1000], Loss: 3175.3989\n",
            "Epoch [230/1000], Loss: 2840.1497\n",
            "Epoch [230/1000], Loss: 2394.7244\n",
            "Epoch [230/1000], Loss: 4511.085\n",
            "Epoch [240/1000], Loss: 2641.062\n",
            "Epoch [240/1000], Loss: 2201.6543\n",
            "Epoch [240/1000], Loss: 3086.9756\n",
            "Epoch [240/1000], Loss: 3276.4553\n",
            "Epoch [240/1000], Loss: 3458.3242\n",
            "Epoch [240/1000], Loss: 2735.4043\n",
            "Epoch [240/1000], Loss: 3039.3252\n",
            "Epoch [240/1000], Loss: 848.8615\n",
            "Epoch [250/1000], Loss: 3180.0315\n",
            "Epoch [250/1000], Loss: 2890.6416\n",
            "Epoch [250/1000], Loss: 3038.51\n",
            "Epoch [250/1000], Loss: 2693.3975\n",
            "Epoch [250/1000], Loss: 3361.9661\n",
            "Epoch [250/1000], Loss: 2823.1531\n",
            "Epoch [250/1000], Loss: 2313.0278\n",
            "Epoch [250/1000], Loss: 3242.6631\n",
            "Epoch [260/1000], Loss: 2635.3157\n",
            "Epoch [260/1000], Loss: 3747.5569\n",
            "Epoch [260/1000], Loss: 2823.0764\n",
            "Epoch [260/1000], Loss: 3111.7673\n",
            "Epoch [260/1000], Loss: 2779.4788\n",
            "Epoch [260/1000], Loss: 2530.7234\n",
            "Epoch [260/1000], Loss: 2805.2861\n",
            "Epoch [260/1000], Loss: 1036.7761\n",
            "Epoch [270/1000], Loss: 2689.3569\n",
            "Epoch [270/1000], Loss: 1926.1287\n",
            "Epoch [270/1000], Loss: 2695.0239\n",
            "Epoch [270/1000], Loss: 2363.616\n",
            "Epoch [270/1000], Loss: 3002.4492\n",
            "Epoch [270/1000], Loss: 4136.4951\n",
            "Epoch [270/1000], Loss: 3533.7793\n",
            "Epoch [270/1000], Loss: 2403.3752\n",
            "Epoch [280/1000], Loss: 2274.3503\n",
            "Epoch [280/1000], Loss: 2630.9614\n",
            "Epoch [280/1000], Loss: 2568.0435\n",
            "Epoch [280/1000], Loss: 2281.6716\n",
            "Epoch [280/1000], Loss: 3070.5122\n",
            "Epoch [280/1000], Loss: 4212.3481\n",
            "Epoch [280/1000], Loss: 3087.0081\n",
            "Epoch [280/1000], Loss: 6267.2832\n",
            "Epoch [290/1000], Loss: 2049.9558\n",
            "Epoch [290/1000], Loss: 3020.6702\n",
            "Epoch [290/1000], Loss: 3011.6018\n",
            "Epoch [290/1000], Loss: 3139.8408\n",
            "Epoch [290/1000], Loss: 2917.7573\n",
            "Epoch [290/1000], Loss: 3242.6162\n",
            "Epoch [290/1000], Loss: 2804.6262\n",
            "Epoch [290/1000], Loss: 4946.7241\n",
            "Epoch [300/1000], Loss: 2523.042\n",
            "Epoch [300/1000], Loss: 3381.8459\n",
            "Epoch [300/1000], Loss: 2934.3193\n",
            "Epoch [300/1000], Loss: 2200.7896\n",
            "Epoch [300/1000], Loss: 3430.3884\n",
            "Epoch [300/1000], Loss: 3045.7327\n",
            "Epoch [300/1000], Loss: 2726.6909\n",
            "Epoch [300/1000], Loss: 4022.189\n",
            "Epoch [310/1000], Loss: 3218.6609\n",
            "Epoch [310/1000], Loss: 1945.1709\n",
            "Epoch [310/1000], Loss: 2821.4224\n",
            "Epoch [310/1000], Loss: 3375.6216\n",
            "Epoch [310/1000], Loss: 2637.1299\n",
            "Epoch [310/1000], Loss: 2575.3618\n",
            "Epoch [310/1000], Loss: 3582.6943\n",
            "Epoch [310/1000], Loss: 5464.6445\n",
            "Epoch [320/1000], Loss: 2473.895\n",
            "Epoch [320/1000], Loss: 3372.2227\n",
            "Epoch [320/1000], Loss: 2405.7866\n",
            "Epoch [320/1000], Loss: 3441.3894\n",
            "Epoch [320/1000], Loss: 3334.1643\n",
            "Epoch [320/1000], Loss: 2293.0491\n",
            "Epoch [320/1000], Loss: 3033.8464\n",
            "Epoch [320/1000], Loss: 2269.3215\n",
            "Epoch [330/1000], Loss: 3112.2112\n",
            "Epoch [330/1000], Loss: 3369.0637\n",
            "Epoch [330/1000], Loss: 2507.8872\n",
            "Epoch [330/1000], Loss: 2364.8035\n",
            "Epoch [330/1000], Loss: 2878.7534\n",
            "Epoch [330/1000], Loss: 2836.6836\n",
            "Epoch [330/1000], Loss: 3288.2449\n",
            "Epoch [330/1000], Loss: 2236.4519\n",
            "Epoch [340/1000], Loss: 2390.1868\n",
            "Epoch [340/1000], Loss: 2309.0359\n",
            "Epoch [340/1000], Loss: 2262.9116\n",
            "Epoch [340/1000], Loss: 4284.2915\n",
            "Epoch [340/1000], Loss: 3334.125\n",
            "Epoch [340/1000], Loss: 2436.5271\n",
            "Epoch [340/1000], Loss: 3434.6765\n",
            "Epoch [340/1000], Loss: 643.4551\n",
            "Epoch [350/1000], Loss: 2644.3208\n",
            "Epoch [350/1000], Loss: 2805.4341\n",
            "Epoch [350/1000], Loss: 4374.0859\n",
            "Epoch [350/1000], Loss: 2969.0562\n",
            "Epoch [350/1000], Loss: 2972.7688\n",
            "Epoch [350/1000], Loss: 2491.3401\n",
            "Epoch [350/1000], Loss: 2162.5681\n",
            "Epoch [350/1000], Loss: 1678.1273\n",
            "Epoch [360/1000], Loss: 3144.9221\n",
            "Epoch [360/1000], Loss: 3421.3762\n",
            "Epoch [360/1000], Loss: 3063.5105\n",
            "Epoch [360/1000], Loss: 3192.4578\n",
            "Epoch [360/1000], Loss: 2093.4924\n",
            "Epoch [360/1000], Loss: 3232.4526\n",
            "Epoch [360/1000], Loss: 2282.5474\n",
            "Epoch [360/1000], Loss: 1254.5212\n",
            "Epoch [370/1000], Loss: 2474.5916\n",
            "Epoch [370/1000], Loss: 4097.5249\n",
            "Epoch [370/1000], Loss: 3085.0344\n",
            "Epoch [370/1000], Loss: 2250.8721\n",
            "Epoch [370/1000], Loss: 3024.7629\n",
            "Epoch [370/1000], Loss: 2357.1787\n",
            "Epoch [370/1000], Loss: 3143.9021\n",
            "Epoch [370/1000], Loss: 940.2873\n",
            "Epoch [380/1000], Loss: 2363.4407\n",
            "Epoch [380/1000], Loss: 3245.9946\n",
            "Epoch [380/1000], Loss: 2910.0161\n",
            "Epoch [380/1000], Loss: 2648.9722\n",
            "Epoch [380/1000], Loss: 3065.7773\n",
            "Epoch [380/1000], Loss: 3017.4612\n",
            "Epoch [380/1000], Loss: 3095.4067\n",
            "Epoch [380/1000], Loss: 2261.958\n",
            "Epoch [390/1000], Loss: 4215.2734\n",
            "Epoch [390/1000], Loss: 2119.6943\n",
            "Epoch [390/1000], Loss: 2823.4712\n",
            "Epoch [390/1000], Loss: 3162.6506\n",
            "Epoch [390/1000], Loss: 2852.0378\n",
            "Epoch [390/1000], Loss: 2952.095\n",
            "Epoch [390/1000], Loss: 2262.8135\n",
            "Epoch [390/1000], Loss: 1618.7816\n",
            "Epoch [400/1000], Loss: 2755.8381\n",
            "Epoch [400/1000], Loss: 3497.0557\n",
            "Epoch [400/1000], Loss: 3187.9868\n",
            "Epoch [400/1000], Loss: 2250.6936\n",
            "Epoch [400/1000], Loss: 2326.3306\n",
            "Epoch [400/1000], Loss: 2835.0718\n",
            "Epoch [400/1000], Loss: 3405.6746\n",
            "Epoch [400/1000], Loss: 3738.8892\n",
            "Epoch [410/1000], Loss: 2949.8774\n",
            "Epoch [410/1000], Loss: 3116.2175\n",
            "Epoch [410/1000], Loss: 2353.2549\n",
            "Epoch [410/1000], Loss: 2681.9683\n",
            "Epoch [410/1000], Loss: 3019.3481\n",
            "Epoch [410/1000], Loss: 3377.2756\n",
            "Epoch [410/1000], Loss: 2804.3572\n",
            "Epoch [410/1000], Loss: 3075.2341\n",
            "Epoch [420/1000], Loss: 2871.2869\n",
            "Epoch [420/1000], Loss: 3611.4692\n",
            "Epoch [420/1000], Loss: 2359.2278\n",
            "Epoch [420/1000], Loss: 3214.7925\n",
            "Epoch [420/1000], Loss: 2742.0774\n",
            "Epoch [420/1000], Loss: 2809.928\n",
            "Epoch [420/1000], Loss: 2808.3577\n",
            "Epoch [420/1000], Loss: 1055.2263\n",
            "Epoch [430/1000], Loss: 3385.23\n",
            "Epoch [430/1000], Loss: 1899.4152\n",
            "Epoch [430/1000], Loss: 2772.8354\n",
            "Epoch [430/1000], Loss: 3855.6731\n",
            "Epoch [430/1000], Loss: 2974.459\n",
            "Epoch [430/1000], Loss: 2572.5759\n",
            "Epoch [430/1000], Loss: 2737.4331\n",
            "Epoch [430/1000], Loss: 4769.8901\n",
            "Epoch [440/1000], Loss: 2803.6975\n",
            "Epoch [440/1000], Loss: 3124.0112\n",
            "Epoch [440/1000], Loss: 1771.0137\n",
            "Epoch [440/1000], Loss: 2803.7903\n",
            "Epoch [440/1000], Loss: 3413.4905\n",
            "Epoch [440/1000], Loss: 2312.9006\n",
            "Epoch [440/1000], Loss: 4165.9863\n",
            "Epoch [440/1000], Loss: 1664.9761\n",
            "Epoch [450/1000], Loss: 3016.9546\n",
            "Epoch [450/1000], Loss: 3134.7192\n",
            "Epoch [450/1000], Loss: 2640.4441\n",
            "Epoch [450/1000], Loss: 3393.5291\n",
            "Epoch [450/1000], Loss: 2535.4954\n",
            "Epoch [450/1000], Loss: 3049.1467\n",
            "Epoch [450/1000], Loss: 2584.1641\n",
            "Epoch [450/1000], Loss: 2112.1611\n",
            "Epoch [460/1000], Loss: 2935.8674\n",
            "Epoch [460/1000], Loss: 3089.8274\n",
            "Epoch [460/1000], Loss: 2585.6033\n",
            "Epoch [460/1000], Loss: 3283.6118\n",
            "Epoch [460/1000], Loss: 3325.5024\n",
            "Epoch [460/1000], Loss: 2645.4587\n",
            "Epoch [460/1000], Loss: 2164.1851\n",
            "Epoch [460/1000], Loss: 7611.7588\n",
            "Epoch [470/1000], Loss: 3103.7534\n",
            "Epoch [470/1000], Loss: 2324.7166\n",
            "Epoch [470/1000], Loss: 2910.5364\n",
            "Epoch [470/1000], Loss: 3282.5457\n",
            "Epoch [470/1000], Loss: 3410.0713\n",
            "Epoch [470/1000], Loss: 2601.6294\n",
            "Epoch [470/1000], Loss: 2677.1731\n",
            "Epoch [470/1000], Loss: 2789.9111\n",
            "Epoch [480/1000], Loss: 3224.4546\n",
            "Epoch [480/1000], Loss: 3461.4131\n",
            "Epoch [480/1000], Loss: 2562.783\n",
            "Epoch [480/1000], Loss: 2841.0762\n",
            "Epoch [480/1000], Loss: 2667.7158\n",
            "Epoch [480/1000], Loss: 2977.6812\n",
            "Epoch [480/1000], Loss: 2679.478\n",
            "Epoch [480/1000], Loss: 1129.4404\n",
            "Epoch [490/1000], Loss: 2784.5981\n",
            "Epoch [490/1000], Loss: 2645.9563\n",
            "Epoch [490/1000], Loss: 2453.3931\n",
            "Epoch [490/1000], Loss: 3727.2036\n",
            "Epoch [490/1000], Loss: 2398.406\n",
            "Epoch [490/1000], Loss: 3343.7686\n",
            "Epoch [490/1000], Loss: 3007.8469\n",
            "Epoch [490/1000], Loss: 2153.158\n",
            "Epoch [500/1000], Loss: 3033.5669\n",
            "Epoch [500/1000], Loss: 2831.7024\n",
            "Epoch [500/1000], Loss: 2287.1697\n",
            "Epoch [500/1000], Loss: 2730.4697\n",
            "Epoch [500/1000], Loss: 3856.4648\n",
            "Epoch [500/1000], Loss: 3116.0422\n",
            "Epoch [500/1000], Loss: 2179.5894\n",
            "Epoch [500/1000], Loss: 7353.2007\n",
            "Epoch [510/1000], Loss: 2285.4443\n",
            "Epoch [510/1000], Loss: 2752.1345\n",
            "Epoch [510/1000], Loss: 3761.8679\n",
            "Epoch [510/1000], Loss: 2743.4475\n",
            "Epoch [510/1000], Loss: 2672.2534\n",
            "Epoch [510/1000], Loss: 2587.6348\n",
            "Epoch [510/1000], Loss: 3626.8967\n",
            "Epoch [510/1000], Loss: 1045.8306\n",
            "Epoch [520/1000], Loss: 4398.0479\n",
            "Epoch [520/1000], Loss: 3164.5286\n",
            "Epoch [520/1000], Loss: 2610.3828\n",
            "Epoch [520/1000], Loss: 2852.8164\n",
            "Epoch [520/1000], Loss: 2655.8511\n",
            "Epoch [520/1000], Loss: 2306.9512\n",
            "Epoch [520/1000], Loss: 2399.6047\n",
            "Epoch [520/1000], Loss: 1471.1687\n",
            "Epoch [530/1000], Loss: 3074.0308\n",
            "Epoch [530/1000], Loss: 3321.5371\n",
            "Epoch [530/1000], Loss: 3295.8662\n",
            "Epoch [530/1000], Loss: 2541.7231\n",
            "Epoch [530/1000], Loss: 2517.6567\n",
            "Epoch [530/1000], Loss: 2937.9036\n",
            "Epoch [530/1000], Loss: 2759.3184\n",
            "Epoch [530/1000], Loss: 537.46\n",
            "Epoch [540/1000], Loss: 2747.2903\n",
            "Epoch [540/1000], Loss: 2896.0493\n",
            "Epoch [540/1000], Loss: 3716.49\n",
            "Epoch [540/1000], Loss: 2387.0569\n",
            "Epoch [540/1000], Loss: 2788.4775\n",
            "Epoch [540/1000], Loss: 3302.9893\n",
            "Epoch [540/1000], Loss: 2562.5356\n",
            "Epoch [540/1000], Loss: 1408.3147\n",
            "Epoch [550/1000], Loss: 2897.0122\n",
            "Epoch [550/1000], Loss: 2022.3683\n",
            "Epoch [550/1000], Loss: 2479.2202\n",
            "Epoch [550/1000], Loss: 3725.9404\n",
            "Epoch [550/1000], Loss: 3038.3162\n",
            "Epoch [550/1000], Loss: 2952.998\n",
            "Epoch [550/1000], Loss: 3157.6343\n",
            "Epoch [550/1000], Loss: 3398.1909\n",
            "Epoch [560/1000], Loss: 2667.2961\n",
            "Epoch [560/1000], Loss: 2646.5725\n",
            "Epoch [560/1000], Loss: 3055.6118\n",
            "Epoch [560/1000], Loss: 2939.4644\n",
            "Epoch [560/1000], Loss: 2802.1665\n",
            "Epoch [560/1000], Loss: 2580.0457\n",
            "Epoch [560/1000], Loss: 3563.5449\n",
            "Epoch [560/1000], Loss: 3606.4783\n",
            "Epoch [570/1000], Loss: 3114.5503\n",
            "Epoch [570/1000], Loss: 2609.6177\n",
            "Epoch [570/1000], Loss: 2800.3181\n",
            "Epoch [570/1000], Loss: 2312.2205\n",
            "Epoch [570/1000], Loss: 2509.7061\n",
            "Epoch [570/1000], Loss: 3999.1943\n",
            "Epoch [570/1000], Loss: 2979.5684\n",
            "Epoch [570/1000], Loss: 2560.5864\n",
            "Epoch [580/1000], Loss: 3614.1963\n",
            "Epoch [580/1000], Loss: 2456.6021\n",
            "Epoch [580/1000], Loss: 2402.8706\n",
            "Epoch [580/1000], Loss: 2337.9436\n",
            "Epoch [580/1000], Loss: 2619.667\n",
            "Epoch [580/1000], Loss: 3439.0479\n",
            "Epoch [580/1000], Loss: 3503.0413\n",
            "Epoch [580/1000], Loss: 1715.2582\n",
            "Epoch [590/1000], Loss: 3399.5535\n",
            "Epoch [590/1000], Loss: 3105.4536\n",
            "Epoch [590/1000], Loss: 2769.3181\n",
            "Epoch [590/1000], Loss: 3340.0625\n",
            "Epoch [590/1000], Loss: 2287.5405\n",
            "Epoch [590/1000], Loss: 2097.9814\n",
            "Epoch [590/1000], Loss: 3259.51\n",
            "Epoch [590/1000], Loss: 3737.0425\n",
            "Epoch [600/1000], Loss: 2884.1824\n",
            "Epoch [600/1000], Loss: 3414.5784\n",
            "Epoch [600/1000], Loss: 3429.1519\n",
            "Epoch [600/1000], Loss: 2781.2581\n",
            "Epoch [600/1000], Loss: 2985.7246\n",
            "Epoch [600/1000], Loss: 2603.9043\n",
            "Epoch [600/1000], Loss: 2312.0461\n",
            "Epoch [600/1000], Loss: 1084.3295\n",
            "Epoch [610/1000], Loss: 2812.7449\n",
            "Epoch [610/1000], Loss: 2604.0759\n",
            "Epoch [610/1000], Loss: 2592.5327\n",
            "Epoch [610/1000], Loss: 3124.4946\n",
            "Epoch [610/1000], Loss: 3729.1726\n",
            "Epoch [610/1000], Loss: 2959.9724\n",
            "Epoch [610/1000], Loss: 2523.9473\n",
            "Epoch [610/1000], Loss: 2215.094\n",
            "Epoch [620/1000], Loss: 2367.4163\n",
            "Epoch [620/1000], Loss: 2838.3069\n",
            "Epoch [620/1000], Loss: 2719.0305\n",
            "Epoch [620/1000], Loss: 3864.2981\n",
            "Epoch [620/1000], Loss: 2288.5161\n",
            "Epoch [620/1000], Loss: 2917.4006\n",
            "Epoch [620/1000], Loss: 3430.8979\n",
            "Epoch [620/1000], Loss: 890.4232\n",
            "Epoch [630/1000], Loss: 2468.1602\n",
            "Epoch [630/1000], Loss: 2858.6106\n",
            "Epoch [630/1000], Loss: 2745.3313\n",
            "Epoch [630/1000], Loss: 2213.4385\n",
            "Epoch [630/1000], Loss: 3344.5325\n",
            "Epoch [630/1000], Loss: 3410.2937\n",
            "Epoch [630/1000], Loss: 3359.614\n",
            "Epoch [630/1000], Loss: 1224.2476\n",
            "Epoch [640/1000], Loss: 2458.8066\n",
            "Epoch [640/1000], Loss: 3288.0542\n",
            "Epoch [640/1000], Loss: 2905.0425\n",
            "Epoch [640/1000], Loss: 2541.5691\n",
            "Epoch [640/1000], Loss: 2920.844\n",
            "Epoch [640/1000], Loss: 2778.6467\n",
            "Epoch [640/1000], Loss: 3468.2566\n",
            "Epoch [640/1000], Loss: 1904.5715\n",
            "Epoch [650/1000], Loss: 2367.8748\n",
            "Epoch [650/1000], Loss: 3198.3542\n",
            "Epoch [650/1000], Loss: 2490.5388\n",
            "Epoch [650/1000], Loss: 3635.9219\n",
            "Epoch [650/1000], Loss: 2140.448\n",
            "Epoch [650/1000], Loss: 3054.1753\n",
            "Epoch [650/1000], Loss: 3200.3816\n",
            "Epoch [650/1000], Loss: 6606.4717\n",
            "Epoch [660/1000], Loss: 2479.7571\n",
            "Epoch [660/1000], Loss: 2770.2859\n",
            "Epoch [660/1000], Loss: 2891.3877\n",
            "Epoch [660/1000], Loss: 3075.0505\n",
            "Epoch [660/1000], Loss: 2314.1797\n",
            "Epoch [660/1000], Loss: 2696.4541\n",
            "Epoch [660/1000], Loss: 4218.02\n",
            "Epoch [660/1000], Loss: 650.4176\n",
            "Epoch [670/1000], Loss: 2646.3086\n",
            "Epoch [670/1000], Loss: 2902.7424\n",
            "Epoch [670/1000], Loss: 2800.8049\n",
            "Epoch [670/1000], Loss: 2957.845\n",
            "Epoch [670/1000], Loss: 3037.353\n",
            "Epoch [670/1000], Loss: 2402.063\n",
            "Epoch [670/1000], Loss: 3327.9575\n",
            "Epoch [670/1000], Loss: 6671.896\n",
            "Epoch [680/1000], Loss: 2822.6367\n",
            "Epoch [680/1000], Loss: 3157.1316\n",
            "Epoch [680/1000], Loss: 2566.2842\n",
            "Epoch [680/1000], Loss: 2576.7454\n",
            "Epoch [680/1000], Loss: 3353.2686\n",
            "Epoch [680/1000], Loss: 2620.0742\n",
            "Epoch [680/1000], Loss: 3110.1245\n",
            "Epoch [680/1000], Loss: 4509.3276\n",
            "Epoch [690/1000], Loss: 3234.155\n",
            "Epoch [690/1000], Loss: 3526.7354\n",
            "Epoch [690/1000], Loss: 2491.7661\n",
            "Epoch [690/1000], Loss: 2560.9634\n",
            "Epoch [690/1000], Loss: 2222.9497\n",
            "Epoch [690/1000], Loss: 4024.019\n",
            "Epoch [690/1000], Loss: 2376.5278\n",
            "Epoch [690/1000], Loss: 863.1714\n",
            "Epoch [700/1000], Loss: 3314.9067\n",
            "Epoch [700/1000], Loss: 3181.1941\n",
            "Epoch [700/1000], Loss: 3754.9866\n",
            "Epoch [700/1000], Loss: 2987.4006\n",
            "Epoch [700/1000], Loss: 2631.2693\n",
            "Epoch [700/1000], Loss: 1858.1343\n",
            "Epoch [700/1000], Loss: 2561.8521\n",
            "Epoch [700/1000], Loss: 3105.9438\n",
            "Epoch [710/1000], Loss: 2848.1843\n",
            "Epoch [710/1000], Loss: 3177.3838\n",
            "Epoch [710/1000], Loss: 3160.2261\n",
            "Epoch [710/1000], Loss: 2280.707\n",
            "Epoch [710/1000], Loss: 2311.2297\n",
            "Epoch [710/1000], Loss: 2918.6965\n",
            "Epoch [710/1000], Loss: 3215.2969\n",
            "Epoch [710/1000], Loss: 9454.9189\n",
            "Epoch [720/1000], Loss: 3976.5271\n",
            "Epoch [720/1000], Loss: 2632.5583\n",
            "Epoch [720/1000], Loss: 3532.8765\n",
            "Epoch [720/1000], Loss: 2659.0938\n",
            "Epoch [720/1000], Loss: 2900.4153\n",
            "Epoch [720/1000], Loss: 2269.8789\n",
            "Epoch [720/1000], Loss: 2308.6184\n",
            "Epoch [720/1000], Loss: 3239.2646\n",
            "Epoch [730/1000], Loss: 2807.7883\n",
            "Epoch [730/1000], Loss: 2782.76\n",
            "Epoch [730/1000], Loss: 3051.0918\n",
            "Epoch [730/1000], Loss: 2547.0886\n",
            "Epoch [730/1000], Loss: 3748.9146\n",
            "Epoch [730/1000], Loss: 2970.7781\n",
            "Epoch [730/1000], Loss: 2354.3933\n",
            "Epoch [730/1000], Loss: 3431.4131\n",
            "Epoch [740/1000], Loss: 3517.3875\n",
            "Epoch [740/1000], Loss: 2880.239\n",
            "Epoch [740/1000], Loss: 3214.3408\n",
            "Epoch [740/1000], Loss: 1788.2196\n",
            "Epoch [740/1000], Loss: 2729.5337\n",
            "Epoch [740/1000], Loss: 3527.0898\n",
            "Epoch [740/1000], Loss: 2575.9646\n",
            "Epoch [740/1000], Loss: 4207.6729\n",
            "Epoch [750/1000], Loss: 3607.5073\n",
            "Epoch [750/1000], Loss: 2681.7224\n",
            "Epoch [750/1000], Loss: 2284.9978\n",
            "Epoch [750/1000], Loss: 2999.1189\n",
            "Epoch [750/1000], Loss: 2697.5569\n",
            "Epoch [750/1000], Loss: 3278.1104\n",
            "Epoch [750/1000], Loss: 2704.27\n",
            "Epoch [750/1000], Loss: 3607.4871\n",
            "Epoch [760/1000], Loss: 3385.1067\n",
            "Epoch [760/1000], Loss: 3049.8564\n",
            "Epoch [760/1000], Loss: 3041.333\n",
            "Epoch [760/1000], Loss: 2799.594\n",
            "Epoch [760/1000], Loss: 2441.5046\n",
            "Epoch [760/1000], Loss: 2315.4226\n",
            "Epoch [760/1000], Loss: 3207.9468\n",
            "Epoch [760/1000], Loss: 3857.7656\n",
            "Epoch [770/1000], Loss: 2735.3621\n",
            "Epoch [770/1000], Loss: 2674.6987\n",
            "Epoch [770/1000], Loss: 2494.459\n",
            "Epoch [770/1000], Loss: 2806.5969\n",
            "Epoch [770/1000], Loss: 2906.4692\n",
            "Epoch [770/1000], Loss: 2340.8164\n",
            "Epoch [770/1000], Loss: 4471.0176\n",
            "Epoch [770/1000], Loss: 792.4211\n",
            "Epoch [780/1000], Loss: 3161.6702\n",
            "Epoch [780/1000], Loss: 3271.3\n",
            "Epoch [780/1000], Loss: 4448.9438\n",
            "Epoch [780/1000], Loss: 3099.0674\n",
            "Epoch [780/1000], Loss: 2327.9575\n",
            "Epoch [780/1000], Loss: 2091.7458\n",
            "Epoch [780/1000], Loss: 2061.6667\n",
            "Epoch [780/1000], Loss: 281.7787\n",
            "Epoch [790/1000], Loss: 3246.9746\n",
            "Epoch [790/1000], Loss: 3295.6111\n",
            "Epoch [790/1000], Loss: 2573.1194\n",
            "Epoch [790/1000], Loss: 2929.5962\n",
            "Epoch [790/1000], Loss: 2354.0105\n",
            "Epoch [790/1000], Loss: 3041.7957\n",
            "Epoch [790/1000], Loss: 3025.5718\n",
            "Epoch [790/1000], Loss: 74.5093\n",
            "Epoch [800/1000], Loss: 2585.7478\n",
            "Epoch [800/1000], Loss: 3119.043\n",
            "Epoch [800/1000], Loss: 1803.4916\n",
            "Epoch [800/1000], Loss: 2894.8643\n",
            "Epoch [800/1000], Loss: 3964.8862\n",
            "Epoch [800/1000], Loss: 2744.3459\n",
            "Epoch [800/1000], Loss: 3326.0073\n",
            "Epoch [800/1000], Loss: 561.7893\n",
            "Epoch [810/1000], Loss: 2224.0928\n",
            "Epoch [810/1000], Loss: 3391.083\n",
            "Epoch [810/1000], Loss: 2112.0198\n",
            "Epoch [810/1000], Loss: 2903.5308\n",
            "Epoch [810/1000], Loss: 3250.325\n",
            "Epoch [810/1000], Loss: 3276.5393\n",
            "Epoch [810/1000], Loss: 3038.3916\n",
            "Epoch [810/1000], Loss: 4686.6763\n",
            "Epoch [820/1000], Loss: 2897.0256\n",
            "Epoch [820/1000], Loss: 3335.0808\n",
            "Epoch [820/1000], Loss: 2983.0786\n",
            "Epoch [820/1000], Loss: 2529.271\n",
            "Epoch [820/1000], Loss: 3357.6855\n",
            "Epoch [820/1000], Loss: 2362.4641\n",
            "Epoch [820/1000], Loss: 2863.6211\n",
            "Epoch [820/1000], Loss: 2850.8855\n",
            "Epoch [830/1000], Loss: 3325.3921\n",
            "Epoch [830/1000], Loss: 3253.499\n",
            "Epoch [830/1000], Loss: 2739.8938\n",
            "Epoch [830/1000], Loss: 2789.7234\n",
            "Epoch [830/1000], Loss: 2239.4141\n",
            "Epoch [830/1000], Loss: 2912.9604\n",
            "Epoch [830/1000], Loss: 2935.9353\n",
            "Epoch [830/1000], Loss: 4598.8604\n",
            "Epoch [840/1000], Loss: 2969.7656\n",
            "Epoch [840/1000], Loss: 3737.7834\n",
            "Epoch [840/1000], Loss: 4019.5698\n",
            "Epoch [840/1000], Loss: 2561.5066\n",
            "Epoch [840/1000], Loss: 2243.9548\n",
            "Epoch [840/1000], Loss: 2253.3142\n",
            "Epoch [840/1000], Loss: 2518.8955\n",
            "Epoch [840/1000], Loss: 2725.0654\n",
            "Epoch [850/1000], Loss: 3063.1274\n",
            "Epoch [850/1000], Loss: 2770.7136\n",
            "Epoch [850/1000], Loss: 2990.2979\n",
            "Epoch [850/1000], Loss: 2912.5469\n",
            "Epoch [850/1000], Loss: 3048.219\n",
            "Epoch [850/1000], Loss: 2376.564\n",
            "Epoch [850/1000], Loss: 2984.6089\n",
            "Epoch [850/1000], Loss: 5396.9482\n",
            "Epoch [860/1000], Loss: 2580.0435\n",
            "Epoch [860/1000], Loss: 2420.6396\n",
            "Epoch [860/1000], Loss: 2508.3682\n",
            "Epoch [860/1000], Loss: 2976.5742\n",
            "Epoch [860/1000], Loss: 3422.0505\n",
            "Epoch [860/1000], Loss: 3655.2556\n",
            "Epoch [860/1000], Loss: 2828.7515\n",
            "Epoch [860/1000], Loss: 1400.8706\n",
            "Epoch [870/1000], Loss: 2838.8843\n",
            "Epoch [870/1000], Loss: 3055.1328\n",
            "Epoch [870/1000], Loss: 2835.7964\n",
            "Epoch [870/1000], Loss: 2895.9075\n",
            "Epoch [870/1000], Loss: 2368.2712\n",
            "Epoch [870/1000], Loss: 4103.9761\n",
            "Epoch [870/1000], Loss: 2354.8647\n",
            "Epoch [870/1000], Loss: 379.6398\n",
            "Epoch [880/1000], Loss: 2989.8542\n",
            "Epoch [880/1000], Loss: 2765.075\n",
            "Epoch [880/1000], Loss: 2561.4229\n",
            "Epoch [880/1000], Loss: 3344.2036\n",
            "Epoch [880/1000], Loss: 3375.9006\n",
            "Epoch [880/1000], Loss: 2160.4409\n",
            "Epoch [880/1000], Loss: 3159.5559\n",
            "Epoch [880/1000], Loss: 1949.2251\n",
            "Epoch [890/1000], Loss: 2736.0271\n",
            "Epoch [890/1000], Loss: 1895.2574\n",
            "Epoch [890/1000], Loss: 3803.1746\n",
            "Epoch [890/1000], Loss: 3239.9714\n",
            "Epoch [890/1000], Loss: 2569.5381\n",
            "Epoch [890/1000], Loss: 3703.4648\n",
            "Epoch [890/1000], Loss: 2381.9841\n",
            "Epoch [890/1000], Loss: 2313.7573\n",
            "Epoch [900/1000], Loss: 2809.7249\n",
            "Epoch [900/1000], Loss: 2877.2175\n",
            "Epoch [900/1000], Loss: 2944.543\n",
            "Epoch [900/1000], Loss: 2755.498\n",
            "Epoch [900/1000], Loss: 3064.1221\n",
            "Epoch [900/1000], Loss: 3032.001\n",
            "Epoch [900/1000], Loss: 2827.0195\n",
            "Epoch [900/1000], Loss: 2711.8506\n",
            "Epoch [910/1000], Loss: 2708.3281\n",
            "Epoch [910/1000], Loss: 3388.6121\n",
            "Epoch [910/1000], Loss: 2771.7019\n",
            "Epoch [910/1000], Loss: 2997.509\n",
            "Epoch [910/1000], Loss: 2622.2292\n",
            "Epoch [910/1000], Loss: 2779.0881\n",
            "Epoch [910/1000], Loss: 3157.4802\n",
            "Epoch [910/1000], Loss: 705.5309\n",
            "Epoch [920/1000], Loss: 3554.6182\n",
            "Epoch [920/1000], Loss: 2825.6189\n",
            "Epoch [920/1000], Loss: 2519.9917\n",
            "Epoch [920/1000], Loss: 3844.2827\n",
            "Epoch [920/1000], Loss: 2442.4355\n",
            "Epoch [920/1000], Loss: 2318.6047\n",
            "Epoch [920/1000], Loss: 2791.8867\n",
            "Epoch [920/1000], Loss: 3073.7119\n",
            "Epoch [930/1000], Loss: 2991.5671\n",
            "Epoch [930/1000], Loss: 3098.6663\n",
            "Epoch [930/1000], Loss: 3492.8376\n",
            "Epoch [930/1000], Loss: 3100.4844\n",
            "Epoch [930/1000], Loss: 3397.5874\n",
            "Epoch [930/1000], Loss: 2473.9597\n",
            "Epoch [930/1000], Loss: 1705.6212\n",
            "Epoch [930/1000], Loss: 3672.4722\n",
            "Epoch [940/1000], Loss: 2859.8696\n",
            "Epoch [940/1000], Loss: 2944.45\n",
            "Epoch [940/1000], Loss: 2997.8284\n",
            "Epoch [940/1000], Loss: 2363.3164\n",
            "Epoch [940/1000], Loss: 3047.0449\n",
            "Epoch [940/1000], Loss: 3123.103\n",
            "Epoch [940/1000], Loss: 3067.2646\n",
            "Epoch [940/1000], Loss: 1272.8134\n",
            "Epoch [950/1000], Loss: 3155.4431\n",
            "Epoch [950/1000], Loss: 3409.5374\n",
            "Epoch [950/1000], Loss: 2721.501\n",
            "Epoch [950/1000], Loss: 2573.1472\n",
            "Epoch [950/1000], Loss: 2622.7227\n",
            "Epoch [950/1000], Loss: 2823.8394\n",
            "Epoch [950/1000], Loss: 3070.6423\n",
            "Epoch [950/1000], Loss: 1644.4789\n",
            "Epoch [960/1000], Loss: 2295.0798\n",
            "Epoch [960/1000], Loss: 2777.6868\n",
            "Epoch [960/1000], Loss: 2787.4006\n",
            "Epoch [960/1000], Loss: 3113.1216\n",
            "Epoch [960/1000], Loss: 3036.8696\n",
            "Epoch [960/1000], Loss: 2862.9668\n",
            "Epoch [960/1000], Loss: 2817.5044\n",
            "Epoch [960/1000], Loss: 12929.4297\n",
            "Epoch [970/1000], Loss: 2297.6885\n",
            "Epoch [970/1000], Loss: 2030.903\n",
            "Epoch [970/1000], Loss: 2503.2456\n",
            "Epoch [970/1000], Loss: 3335.4456\n",
            "Epoch [970/1000], Loss: 3096.884\n",
            "Epoch [970/1000], Loss: 3470.5608\n",
            "Epoch [970/1000], Loss: 3598.3831\n",
            "Epoch [970/1000], Loss: 2246.02\n",
            "Epoch [980/1000], Loss: 2536.3828\n",
            "Epoch [980/1000], Loss: 3305.4233\n",
            "Epoch [980/1000], Loss: 2662.6963\n",
            "Epoch [980/1000], Loss: 3526.0286\n",
            "Epoch [980/1000], Loss: 2673.7625\n",
            "Epoch [980/1000], Loss: 2531.1541\n",
            "Epoch [980/1000], Loss: 2831.4124\n",
            "Epoch [980/1000], Loss: 6601.0249\n",
            "Epoch [990/1000], Loss: 3872.2065\n",
            "Epoch [990/1000], Loss: 2626.8967\n",
            "Epoch [990/1000], Loss: 2638.0193\n",
            "Epoch [990/1000], Loss: 2317.8909\n",
            "Epoch [990/1000], Loss: 2706.7939\n",
            "Epoch [990/1000], Loss: 2360.7673\n",
            "Epoch [990/1000], Loss: 3721.4924\n",
            "Epoch [990/1000], Loss: 3744.9019\n",
            "Epoch [1000/1000], Loss: 3605.0093\n",
            "Epoch [1000/1000], Loss: 2312.27\n",
            "Epoch [1000/1000], Loss: 3424.5117\n",
            "Epoch [1000/1000], Loss: 2444.1296\n",
            "Epoch [1000/1000], Loss: 3006.5293\n",
            "Epoch [1000/1000], Loss: 3098.9253\n",
            "Epoch [1000/1000], Loss: 2346.4009\n",
            "Epoch [1000/1000], Loss: 3821.4089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# List to store MSE values for each batch\n",
        "mse_values = []\n",
        "\n",
        "# Disable gradient calculations for inference\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        # Move data to the appropriate device (CPU/GPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Make predictions\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate Mean Squared Error for the batch\n",
        "        mse = criterion(outputs, targets)\n",
        "        mse_values.append(mse.item())\n",
        "\n",
        "# Calculate and print the average MSE across all test batches\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw1yj7T294vG",
        "outputId": "e1522b3a-a488-4a12-9015-26ee3bacea25"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2857.77099609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yv_OPycD-WVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}